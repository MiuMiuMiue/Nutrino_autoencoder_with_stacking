{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wYhK7aJzhSWN",
        "outputId": "7963734f-20de-47b5-9205-7d300e407b06"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import time\n",
        "# from google.colab import drive\n",
        "import psutil\n",
        "\n",
        "\n",
        "seed = 1234\n",
        "tf.random.set_seed(seed)\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "FZ4L66L5hSWO"
      },
      "outputs": [],
      "source": [
        "def preprocess_images(images):\n",
        "  imgs = np.zeros((images.shape[0], 100, 160))\n",
        "  for i in range(images.shape[0]):\n",
        "    imgs[i] = np.hstack((images[i, 0], images[i, 1]))\n",
        "  imgs = imgs.reshape((images.shape[0], 100, 160, 1)) / 255.\n",
        "  return np.where(imgs > .001, 1.0, 0.0).astype('float32')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "IIY2y0JshSWO"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'images.npy'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32md:\\UCI_documents\\2023_Spring_Quarter\\nutrino_autoencoder\\nutrino_autoencoder_with_stacking.ipynb Cell 3\u001b[0m in \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/UCI_documents/2023_Spring_Quarter/nutrino_autoencoder/nutrino_autoencoder_with_stacking.ipynb#W2sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m images \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mload(\u001b[39m'\u001b[39m\u001b[39mimages.npy\u001b[39m\u001b[39m'\u001b[39m, mmap_mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/UCI_documents/2023_Spring_Quarter/nutrino_autoencoder/nutrino_autoencoder_with_stacking.ipynb#W2sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m targets \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mload(\u001b[39m'\u001b[39m\u001b[39mtargets.npy\u001b[39m\u001b[39m'\u001b[39m, mmap_mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\yuanz\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\lib\\npyio.py:390\u001b[0m, in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[0;32m    388\u001b[0m     own_fid \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    389\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 390\u001b[0m     fid \u001b[39m=\u001b[39m stack\u001b[39m.\u001b[39menter_context(\u001b[39mopen\u001b[39;49m(os_fspath(file), \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[0;32m    391\u001b[0m     own_fid \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    393\u001b[0m \u001b[39m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
            "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'images.npy'"
          ]
        }
      ],
      "source": [
        "images = np.load('images.npy', mmap_mode=\"r\")\n",
        "targets = np.load('targets.npy', mmap_mode=\"r\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PVgMkQ7HhSWO",
        "outputId": "e61d01ac-99aa-45cd-c0da-c9c6c7606642"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(6648700, 2, 100, 80)"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "images.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 703
        },
        "id": "nfNwFKoehSWP",
        "outputId": "2b6d9383-b35a-4b1e-e92a-7bb1c08dab56"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'images' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32md:\\UCI_documents\\2023_Spring_Quarter\\nutrino_autoencoder\\nutrino_autoencoder_with_stacking.ipynb Cell 5\u001b[0m in \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/UCI_documents/2023_Spring_Quarter/nutrino_autoencoder/nutrino_autoencoder_with_stacking.ipynb#W4sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m ind \u001b[39m=\u001b[39m \u001b[39m5\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/UCI_documents/2023_Spring_Quarter/nutrino_autoencoder/nutrino_autoencoder_with_stacking.ipynb#W4sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m imgs \u001b[39m=\u001b[39m images[:\u001b[39m32\u001b[39m]\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/UCI_documents/2023_Spring_Quarter/nutrino_autoencoder/nutrino_autoencoder_with_stacking.ipynb#W4sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m fig, axes \u001b[39m=\u001b[39m plt\u001b[39m.\u001b[39msubplots(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/UCI_documents/2023_Spring_Quarter/nutrino_autoencoder/nutrino_autoencoder_with_stacking.ipynb#W4sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m axes[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mimshow(imgs[ind, \u001b[39m0\u001b[39m], cmap\u001b[39m=\u001b[39mplt\u001b[39m.\u001b[39mcm\u001b[39m.\u001b[39mgray)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'images' is not defined"
          ]
        }
      ],
      "source": [
        "ind = 5\n",
        "imgs = images[:32]\n",
        "fig, axes = plt.subplots(1, 2)\n",
        "axes[0].imshow(imgs[ind, 0], cmap=plt.cm.gray)\n",
        "axes[1].imshow(imgs[ind, 1], cmap=plt.cm.gray)\n",
        "plt.show()\n",
        "\n",
        "processed_imgs = preprocess_images(imgs)\n",
        "plt.imshow(processed_imgs[ind], cmap=plt.cm.gray)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "class convBlock(tf.keras.layers.Layer):\n",
        "    def __init__(self, filters, size=3):\n",
        "        super(convBlock, self).__init__()\n",
        "        self.conv1 = tf.keras.layers.Conv2D(filters=filters, kernel_size=size, padding='same')\n",
        "        self.conv2 = tf.keras.layers.Conv2D(filters=2 * filters, kernel_size=1)\n",
        "        self.conv3 = tf.keras.layers.Conv2D(filters=filters, kernel_size=1)\n",
        "        self.activation = tf.keras.layers.ReLU()\n",
        "\n",
        "    def call(self, inputs):\n",
        "        result = self.conv1(inputs)\n",
        "        result = self.conv2(result)\n",
        "        result = self.activation(result)\n",
        "        result = self.conv3(result)\n",
        "        # result = self.activation(result)\n",
        "        \n",
        "        return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "WkkF6PLRhSWP"
      },
      "outputs": [],
      "source": [
        "class convolutionalVariationalAutoencoder(tf.keras.Model):\n",
        "    def __init__(self, latent_dim):\n",
        "        super(convolutionalVariationalAutoencoder, self).__init__(latent_dim)\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "        self.encoder = tf.keras.Sequential([\n",
        "            tf.keras.layers.InputLayer(input_shape=(100, 160, 1)), \n",
        "            tf.keras.layers.Conv2D(filters=32, kernel_size=3), \n",
        "            convBlock(filters=32, size=3),\n",
        "            convBlock(filters=32, size=3),\n",
        "            # convBlock(filters=32, size=3),\n",
        "            tf.keras.layers.Conv2D(filters=32, kernel_size=3), \n",
        "            convBlock(filters=32, size=3), \n",
        "            convBlock(filters=32, size=3), \n",
        "            # convBlock(filters=32, size=3), \n",
        "            tf.keras.layers.Conv2D(filters=32, kernel_size=3), \n",
        "            convBlock(filters=32, size=3), \n",
        "            convBlock(filters=32, size=3),\n",
        "            # convBlock(filters=32, size=3), \n",
        "            tf.keras.layers.Conv2D(filters=64, kernel_size=3), \n",
        "            convBlock(filters=64, size=3), \n",
        "            convBlock(filters=64, size=3),\n",
        "            tf.keras.layers.Conv2D(filters=64, kernel_size=3), \n",
        "            convBlock(filters=64, size=3), \n",
        "            convBlock(filters=64, size=3),\n",
        "            tf.keras.layers.Flatten(),\n",
        "            tf.keras.layers.Dense(128),\n",
        "            tf.keras.layers.Dense(64), \n",
        "            tf.keras.layers.Dense(latent_dim + latent_dim)\n",
        "        ])\n",
        "\n",
        "        self.decoder = tf.keras.Sequential([\n",
        "            tf.keras.layers.InputLayer(input_shape=(latent_dim, )), \n",
        "            tf.keras.layers.Dense(64), \n",
        "            tf.keras.layers.Dense(128),\n",
        "            tf.keras.layers.Dense(90 * 150 * 128, activation=\"relu\"), \n",
        "            tf.keras.layers.Reshape(target_shape=(90, 150, 128)),\n",
        "            tf.keras.layers.Conv2DTranspose(filters=64, kernel_size=3),\n",
        "            convBlock(filters=32, size=3),\n",
        "            convBlock(filters=32, size=3),\n",
        "            tf.keras.layers.Conv2DTranspose(filters=32, kernel_size=3),\n",
        "            convBlock(filters=32, size=3),\n",
        "            convBlock(filters=32, size=3),\n",
        "            tf.keras.layers.Conv2DTranspose(filters=64, kernel_size=3),\n",
        "            convBlock(filters=32, size=3),\n",
        "            convBlock(filters=32, size=3),\n",
        "            tf.keras.layers.Conv2DTranspose(filters=32, kernel_size=3, activation='relu'),\n",
        "            convBlock(filters=32, size=3),\n",
        "            convBlock(filters=32, size=3),\n",
        "            tf.keras.layers.Conv2DTranspose(filters=64, kernel_size=3, activation='relu'),\n",
        "            convBlock(filters=32, size=3),\n",
        "            convBlock(filters=32, size=3),\n",
        "            tf.keras.layers.Conv2DTranspose(filters=1, kernel_size=3, strides=1, padding='same')\n",
        "        ])\n",
        "\n",
        "    def sample(self, eps=None):\n",
        "        if eps is None:\n",
        "            eps = tf.random.normal(shape=(100, self.latent_dim))\n",
        "        return self.decode(eps, apply_sigmoid=True)\n",
        "    \n",
        "    @tf.function\n",
        "    def encode(self, x):\n",
        "        mean, logvar = tf.split(self.encoder(x), num_or_size_splits=2, axis=1)\n",
        "        return mean, logvar\n",
        "    \n",
        "    def reparameterize(self, mean, logvar):\n",
        "        eps = tf.random.normal(shape=mean.shape)\n",
        "        return eps * tf.exp(logvar * 0.5) + mean\n",
        "    \n",
        "    @tf.function\n",
        "    def decode(self, z, apply_sigmoid=False):\n",
        "        logits = self.decoder(z)\n",
        "        if apply_sigmoid:\n",
        "            probs = tf.sigmoid(logits)\n",
        "            return probs\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = convolutionalVariationalAutoencoder(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_12\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_228 (Conv2D)         (None, 98, 158, 32)       320       \n",
            "                                                                 \n",
            " conv_block_66 (convBlock)   (None, 98, 158, 32)       13440     \n",
            "                                                                 \n",
            " conv_block_67 (convBlock)   (None, 98, 158, 32)       13440     \n",
            "                                                                 \n",
            " conv2d_235 (Conv2D)         (None, 96, 156, 32)       9248      \n",
            "                                                                 \n",
            " conv_block_68 (convBlock)   (None, 96, 156, 32)       13440     \n",
            "                                                                 \n",
            " conv_block_69 (convBlock)   (None, 96, 156, 32)       13440     \n",
            "                                                                 \n",
            " conv2d_242 (Conv2D)         (None, 94, 154, 32)       9248      \n",
            "                                                                 \n",
            " conv_block_70 (convBlock)   (None, 94, 154, 32)       13440     \n",
            "                                                                 \n",
            " conv_block_71 (convBlock)   (None, 94, 154, 32)       13440     \n",
            "                                                                 \n",
            " conv2d_249 (Conv2D)         (None, 92, 152, 64)       18496     \n",
            "                                                                 \n",
            " conv_block_72 (convBlock)   (None, 92, 152, 64)       53504     \n",
            "                                                                 \n",
            " conv_block_73 (convBlock)   (None, 92, 152, 64)       53504     \n",
            "                                                                 \n",
            " conv2d_256 (Conv2D)         (None, 90, 150, 64)       36928     \n",
            "                                                                 \n",
            " conv_block_74 (convBlock)   (None, 90, 150, 64)       53504     \n",
            "                                                                 \n",
            " conv_block_75 (convBlock)   (None, 90, 150, 64)       53504     \n",
            "                                                                 \n",
            " flatten_6 (Flatten)         (None, 864000)            0         \n",
            "                                                                 \n",
            " dense_35 (Dense)            (None, 128)               110592128 \n",
            "                                                                 \n",
            " dense_36 (Dense)            (None, 64)                8256      \n",
            "                                                                 \n",
            " dense_37 (Dense)            (None, 20)                1300      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 110,970,580\n",
            "Trainable params: 110,970,580\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.encoder.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_13\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_38 (Dense)            (None, 64)                704       \n",
            "                                                                 \n",
            " dense_39 (Dense)            (None, 128)               8320      \n",
            "                                                                 \n",
            " dense_40 (Dense)            (None, 1728000)           222912000 \n",
            "                                                                 \n",
            " reshape_6 (Reshape)         (None, 90, 150, 128)      0         \n",
            "                                                                 \n",
            " conv2d_transpose_33 (Conv2D  (None, 92, 152, 64)      73792     \n",
            " Transpose)                                                      \n",
            "                                                                 \n",
            " conv_block_76 (convBlock)   (None, 92, 152, 32)       22656     \n",
            "                                                                 \n",
            " conv_block_77 (convBlock)   (None, 92, 152, 32)       13440     \n",
            "                                                                 \n",
            " conv2d_transpose_34 (Conv2D  (None, 94, 154, 32)      9248      \n",
            " Transpose)                                                      \n",
            "                                                                 \n",
            " conv_block_78 (convBlock)   (None, 94, 154, 32)       13440     \n",
            "                                                                 \n",
            " conv_block_79 (convBlock)   (None, 94, 154, 32)       13440     \n",
            "                                                                 \n",
            " conv2d_transpose_35 (Conv2D  (None, 96, 156, 64)      18496     \n",
            " Transpose)                                                      \n",
            "                                                                 \n",
            " conv_block_80 (convBlock)   (None, 96, 156, 32)       22656     \n",
            "                                                                 \n",
            " conv_block_81 (convBlock)   (None, 96, 156, 32)       13440     \n",
            "                                                                 \n",
            " conv2d_transpose_36 (Conv2D  (None, 98, 158, 32)      9248      \n",
            " Transpose)                                                      \n",
            "                                                                 \n",
            " conv_block_82 (convBlock)   (None, 98, 158, 32)       13440     \n",
            "                                                                 \n",
            " conv_block_83 (convBlock)   (None, 98, 158, 32)       13440     \n",
            "                                                                 \n",
            " conv2d_transpose_37 (Conv2D  (None, 100, 160, 64)     18496     \n",
            " Transpose)                                                      \n",
            "                                                                 \n",
            " conv_block_84 (convBlock)   (None, 100, 160, 32)      22656     \n",
            "                                                                 \n",
            " conv_block_85 (convBlock)   (None, 100, 160, 32)      13440     \n",
            "                                                                 \n",
            " conv2d_transpose_38 (Conv2D  (None, 100, 160, 1)      289       \n",
            " Transpose)                                                      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 223,212,641\n",
            "Trainable params: 223,212,641\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.decoder.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "SUdcLHsthSWP"
      },
      "outputs": [],
      "source": [
        "optimizer = tf.keras.optimizers.Adam(1e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "rRq3PzNthSWP"
      },
      "outputs": [],
      "source": [
        "def kl_divergence(mean, logvar, raxis=1):\n",
        "    kl = -0.5 * tf.reduce_mean(1 + logvar - tf.square(mean) - tf.exp(logvar), axis=raxis)\n",
        "    return kl\n",
        "\n",
        "\n",
        "def compute_loss(model, x, beta):\n",
        "    mean, logvar = model.encode(x)\n",
        "    z = model.reparameterize(mean, logvar)\n",
        "    x_logit = model.sample(z)\n",
        "    reconstruction_loss = tf.keras.losses.binary_crossentropy(x, x_logit, from_logits=True)\n",
        "    reconstruction_loss = tf.reduce_mean(reconstruction_loss)\n",
        "    KL = kl_divergence(mean, logvar)\n",
        "    return tf.reduce_mean(reconstruction_loss + KL * beta)\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def train_step(model, x, optimizer, beta):\n",
        "    \"\"\"Executes one training step and returns the loss.\n",
        "\n",
        "    This function computes the loss and gradients, and uses the latter to\n",
        "    update the model's parameters.\n",
        "    \"\"\"\n",
        "    with tf.GradientTape() as tape:\n",
        "      loss = compute_loss(model, x, beta)\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "d4GxjU2WhSWQ"
      },
      "outputs": [],
      "source": [
        "latent_dim = 30\n",
        "beta = 1e-4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "epoch_num = 0\n",
        "min_loss = 999999999999\n",
        "record = []\n",
        "best_model = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "DO50vdqShSWQ",
        "outputId": "619c3cb9-1fb2-479e-994d-4cd06c7c604b"
      },
      "outputs": [],
      "source": [
        "for epochs in range(30, 120, 20):\n",
        "    model = convolutionalVariationalAutoencoder(latent_dim)\n",
        "    print(f\"Epoch {epochs}:\")\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "\n",
        "        start_time = time.time()\n",
        "        print(\"\\tTrain: \", end=\"\")\n",
        "        num = 1\n",
        "        train_loss = tf.keras.metrics.Mean()\n",
        "\n",
        "        for train_ind in range(0, 5318784, 128):\n",
        "            train_x = preprocess_images(images[train_ind: train_ind + 128])\n",
        "            loss_value = train_step(model, train_x, optimizer, beta)\n",
        "            train_loss(loss_value)\n",
        "            if train_ind // 53187 >= num:\n",
        "                print(\"=\", end=\"\")\n",
        "                num += 1\n",
        "        print()\n",
        "\n",
        "        train_loss = train_loss.result()\n",
        "        end_time = time.time()\n",
        "\n",
        "        print(f\"\\tEpoch: {epoch}, Train set Loss: {train_loss}, time elapse for current epoch: {(end_time - start_time) / 60:.2f}\")\n",
        "\n",
        "\n",
        "    print(\"\\tTesting: \", end=\"\")\n",
        "    num = 1\n",
        "    test_loss = tf.keras.metrics.Mean()\n",
        "    for test_ind in range(0, 1329792, 128):\n",
        "        test_x = preprocess_images(images[5318784 + test_ind: 5318784 + test_ind + 128])\n",
        "        test_loss(compute_loss(model, test_x, beta))\n",
        "        if test_ind // 13297 >= num:\n",
        "            print(\"=\", end=\"\")\n",
        "            num += 1\n",
        "    print()\n",
        "    test_loss = test_loss.result()\n",
        "    print(f\"\\tEpoch: {epoch}, Test set Loss: {test_loss}\")\n",
        "\n",
        "    record.append((epochs, test_loss))\n",
        "    if test_loss < min_loss:\n",
        "        min_loss = test_loss\n",
        "        epoch_num = epochs\n",
        "        best_model = model\n",
        "    model.encoder.save_weights(f\"model_encoder_epoch_{epochs}.h5\")\n",
        "    model.decoder.save_weights(f\"model_decoder_epoch_{epochs}.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"Best Epoch = {epoch_num} with loss = {min_loss}\")\n",
        "with open(\"epochsRecord.txt\", \"w\") as writer:\n",
        "    for data in record:\n",
        "        writer.write(f\"Epochs = {data[0]}, Loss = {data[1]}\\n\")\n",
        "    writer.write(f\"Best Epoch = {epoch_num} with loss = {min_loss}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N6Xk-kXhxU_8"
      },
      "outputs": [],
      "source": [
        "ind = 6648000\n",
        "# ind = 0\n",
        "imgs = preprocess_images(images[ind: ind + 32])\n",
        "\n",
        "num = 4\n",
        "\n",
        "plt.imshow(imgs[num], cmap=plt.cm.gray)\n",
        "plt.show()\n",
        "\n",
        "mean, logvar = best_model.encode(np.array([imgs[num]]))\n",
        "z = best_model.reparameterize(mean, logvar)\n",
        "x_logit = best_model.decode(z)[0]\n",
        "\n",
        "plt.imshow(x_logit, cmap=plt.cm.gray)\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
